# -*- coding: utf-8 -*-
"""nlp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UTxxd1sgAr1fecyRYx_dlF7tLYh0Yk8B
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np

from sklearn.decomposition import PCA, TruncatedSVD
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import seaborn as sns

import plotly.graph_objects as go
import warnings
from gensim.models import word2vec, FastText
import re
import string
import nltk
from nltk.corpus import stopwords

# !pip install gensim
import pandas as pd
import gensim
from gensim.test.utils import datapath, get_tmpfile
from gensim.models import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec

nltk.download('stopwords')
nltk.download('punkt')

stopwords.words("english")[:10]

"""## **We read the dataset**

## We have not able to train the combine four data because of our machine capacity; even one data. So for our exersice, we will take a sample of words(paragraphs) in one of our data
"""

with open('/content/drive/MyDrive/Assign1/Text Data/39185-0.txt') as file:
  df=file.read()
  print(df)

with open('/content/drive/MyDrive/Assign1/Text Data/64553-0.txt') as file1:
  df1=file1.read()
  #print(df1)

with open('/content/drive/MyDrive/Assign1/Text Data/pg5891.txt') as file2:
  df2=file2.read()
  #print(df2)

with open('/content/drive/MyDrive/Assign1/Text Data/pg66923.txt') as file3:
  df3=file3.read()
  #print(df3)

len(df1), len(df2), len(df3)

"""## **We take a sample of data in 2nd data(df1) with which we will work and call it text.**"""

#we can also took a sample in a data as df3[:2000] for example
text="""
From Dakka the boundary line follows the Kam for about a dozen miles,
and then, leaving the river, it runs over a block of hills which form
the fringe of a vast unknown tract of the Cameroon country. Here the
hill-top villages are few, the inhabitants are wilder and more squalid
than the Dakka natives, and the land is the haunt of the elephant, the
lion, the bush-cow and the leopard. From these hills the boundary
descends into the valley of the River Lumen, which runs for twenty or
thirty miles under a dark arch of overhanging trees. The water of the
Lumen is very cold, even in the heat of the day, and the sands of the
river are full of iron. The line crosses the Lumen and mounts a high
ridge, called Shina, to descend again into the vast plain of the River
Teraba. Along the banks of the Teraba are numerous Hausa and Jukum
villages, situated on important trade roads between Northern Nigeria and
Cameroon, the principal trade being in rubber, kola nuts, sheep, and
goats. There are no cattle, as many kinds of biting fly, including the
tsetse, have their breeding places in this area. As the Teraba is
typical of all the great southern tributaries of the Benue, the
following short description, which Captain Nugent gives of one of the
upper reaches, will be read with interest:--

“Fifteen miles above Karbabi the river bends sharply at right angles,
forming noisy rapids. Above the rapids the bed is rocky with deep pools.
Under the tall trees along the banks are open glades like an English
beech wood, entirely free from undergrowth, the ground being carpeted
with soft moss. There are the feeding-grounds of huge herds of
hippopotami, who live in the pools in the daytime. The river is here 200
to 300 yards wide, with high banks; the channel winds among huge
boulders, forming a chain of pools, but leaving a narrow deep waterway
among the larger rocks. The pools are like dark mirrors, silent and
stagnant, yet bright and clear, reflecting the trees on the opposite
bank in full detail. Wild geese and ibis fly overhead, whilst large
alligators move about like torpedoes, with their noses out of the water,
leaving long trails of bubbles on the surface.

“There is no village within many miles of this place, and it was only
with the greatest difficulty that we could obtain guides, as there are
no tracks except those made by the larger game. The inhabitants of the
pools were thoroughly startled at our approach. There seemed to be a
sort of collusion between the different birds and beasts. The shrieking
ibis warned the alligators asleep on the rocks in the sun, they, in
alarm, slid into the water and warned the river-horse that something was
amiss; the river-horse in his turn went pounding up-stream, under water,
coming up to breathe at intervals behind the rocks and branches. The
snorting was terrific. We estimated that there were between thirty and
forty hippopotami in the largest pool. I have never seen a
wilder-looking place; it seemed to be alive with everything except
humanity.


IN THE CANNIBAL COUNTRY.

“The boundary after crossing the Gazuba River, a tributary of the
Teraba, again ascends into an unexplored continuation of the Banjo
highlands, and drops into the plain of the Donga Valley. The inhabitants
here are a mixture of Jukums and Zumperis, but there are numerous
settlements of Hausas, whose trade consists of smuggling rubber and kola
nuts into Nigeria without paying the German tax. The pagans, who live in
‘swallow-nest’ villages on the heights, cultivate guinea-corn and root
crops, while yams, cassava and sweet potatoes grow in abundance in the
interstices between the huts. The boundary reaches the Donga, and after
following the river for fifteen miles and crossing the plateau of the
Wanya Mountains, reaches the plain of the Bamana Valley, in which oil
palms are first encountered.

“The country between the Gamana and Katsena Rivers is inhabited by
Zumperi pagans, who are cannibals and live on hill-tops. They are of
small stature and of remarkably repulsive appearance. Every other man
appeared to be suffering from goitre or elephantiasis--whether the
legacy of cannibalism, or the effect of drinking infected water, it is
difficult to say. The people are industrious, and besides corn, grow
large quantities of cotton and tobacco on the hillsides. They breed dogs
for eating purposes, and all the villages are full of yelping curs,
covered with sores like their owners. In one village a large deposit of
human skulls was seen. The villages are well built and surrounded by mud
walls and ditches. Among the numerous ‘ju-jus’ found in the deserted
huts was a grotesque mask, which was apparently kept to frighten the
women. Any woman seeing it must die at once. When the community is short
of meat, the local witch doctor puts on the mask and runs about the
hills until he meets a likely looking victim, who is then killed and
eaten. The Zumperis are great hunters, and have killed off nearly all
the game in their country except leopards.”
"""

text

len(text)

"""## Regardless of language, this should be processed before being delivered to the Word2Vec model. We have to go and remove the english stopwords, clean up punctuation, numbers and other symbols.

## **Preprocessing**

### Tokenization
"""

import re
from string import punctuation 

def tokenize(text):
    pattern = re.compile(r'[A-Za-z]+[\w^\']*|[\w^\']*[A-Za-z]+[\w^\']*')
    return pattern.findall(text.lower())
    # The function mapping  map each word of our tokens to an indice 
    #such that we can not have 2 time the same words
def mapping(tokens):
    '''create a map between tokens and indices'''
    word_to_id = {}
    id_to_word = {}
    
    for i, token in enumerate(set(tokens)):
        word_to_id[token] = i
        id_to_word[i] = token
    
    return word_to_id, id_to_word

"""## After cleaning our text we obtain less number of words and we remark that those remove are significant(so preprocessing is very important step in order to have a good result.) """

tokens = tokenize(text)

len(tokens)

"""## We can observe that the number of words reduce from 5008 to 867 after applying preprocessing. """

tokens

"""## **But when we observe the tokens words, we can see that the words as: "are, is, and, a, in, the, he, on, of, to..."  appear repeatively but the are non informative. To make a good model, we will reduce those words(stopwords) before contnuous.**"""

tokens=[w.lower().strip() for w in tokens if not w.lower() in stopwords.words("english")]

len(tokens)

"""## **After removing the stopwords, we see that again the words recude, by removing 399 other words(from 867 to 468).**"""

#tokens

word_to_idex, idex_to_word = mapping(tokens)
word_to_idex

#idex_to_word

"""## With these index, we can see that we have 349 (because index start from 0 to 348) words; that means 349 unique words in our vocabulary.

### Generate training Data
- We know tokens are strings, we need to encode them numerically using one-hot vectorization. 
- We also need to generate the input and target values.
- we need to also create the context of each word
"""

np.random.seed(123)
def generate_training_data(tokens, word_to_idex, window):
    X = []
    y = []
    n_tokens = len(tokens)
    for i in range(n_tokens):
        idx = concat(
            range(max(0, i - window), i), 
            range(i, min(n_tokens, i + window + 1))
        )
        for j in idx:
            if i == j:
                continue
            X.append(one_hot_encode(word_to_idex[tokens[i]], len(word_to_idex)))
            y.append(one_hot_encode(word_to_idex[tokens[j]], len(word_to_idex)))
    
    return np.asarray(X), np.asarray(y)


def concat(*iterables):
    for iterable in iterables:
        yield from iterable


def one_hot_encode(id, vocab_size):
    res = [0] * vocab_size
    res[id] = 1
    return res

"""## In the code above we have X and y matrix.
## **X** represent the matrix that share the number of context of each word (in 468 before indexing). In this code we see clairly that the first and last words can not have the same number of words in their context as other.


## **y** represent the matrix that share the corresponding context of each word present in **X**. 
"""

X, y = generate_training_data(tokens, word_to_idex, 3)

"""### We take our context or window equal to 3; function of our context set, more words we have, larger will be the size of **X** and **y** matrix. 

### The number of columns is already fixed through the mapping function where we index each word and it is **349** (so each word is one-hot vector of 349 elements), the size of our 2 matrix must be the same. Why? If we take *468* words, the 1st word will have 3 in it context, the 2nd 4 and the 3rd 5 as the same with the last 3, 2nd last 4 and 3 last 5. Except those 6 words all willhave 6 words in it context that means the length will be ((468-6)*6)+(2*3 + 2*4 + 2*5) =2796.

### Each word in **X** will map a context word in **y**, that is why the must be the same size

### let take an example: have 3 as window means for most words, 3 words are before and after the concerned words. So we obtain context of 6 for these words. 
"""

X.shape

X

"""To understand well let take those cells below as examples. 

The X[0], X[1], X[2] have the same value that means the first representative word has 3 words in it context but, when we read the y outpout, y[0], y[1],and y[2] are all differents because y[0] correspond for the first context, y[1] for the 2nd an y[3] the 3rd and so on for all words. 
"""

X[0]

#X[1]

X[2]

X[3]

#X[6],X[7],X[8]

#X[7]

#X[10]

#X[11]

#X[12]

y[0]

y[1]

y.shape

#X[335]

#idex_to_word[238]

"""Both X and y are matrices with 2796 rows and 349 columns. 
- 2796 is the number of training examples we have (this number to have been larger had we used a larger window).
- 349 is the number of unique tokens we have in the original text.  
Since we have one-hot encoded both the input and output as 349-dimensional sparse vectors, this is expected.

## **Embedding Model**

The representation we will use for the model is a Python dictionary, whose values are the weight matrices and keys, the name with which we will refer to the weight matrices "W1" and "W2" to refer to these weights.  
We initialize W1 and W2 as onehot encoded matrices
"""

def init_network(vocab_size, n_embedding):
    model = {
        "w1": np.random.randn(vocab_size, n_embedding),
        "w2": np.random.randn(n_embedding, vocab_size)
    }
    return model

"""The embeddings are the rows of the first weight matrix (W1).  
During training, we adjust the weights of W1, along with the weight matrix in the second layer (W2), using cross entropy loss.  
A1 = XW1  
A2 = A1W2  
Z = softmax(A2)  

where Z is the matrix contains the prediction probability vectors.  
The whole Idea is to train a classificatiom model to predict A (the onehot encoded vector of the target words)
"""

model = init_network(len(word_to_idex), 10)

model["w1"]

#model["w2"]

"""## **Forward Propagation**"""

def forward(model, X, return_cache=True):
    cache = {}
    cache["a1"] = X @ model["w1"]
    cache["a2"] = cache["a1"] @ model["w2"]
    cache["z"] = softmax(cache["a2"])
    
    if not return_cache:
        return cache["z"]
    
    return cache

# check the dimensionality of the matrices
(X @ model["w1"]).shape

"""All the 2796 tokens in the text to be converted into 10-dimensional vectors"""

# check the dimensionality after passing through the second layer
(X @ model["w1"] @ model["w2"]).shape

"""## **Backpropagation**"""

def backward(model, X, y, alpha):
    
    cache  = forward(model, X)

    da2 = cache["z"] - y
    dw2 = cache["a1"].T @ da2
    
    da1 = da2 @ model["w2"].T
    dw1 = X.T @ da1
    
    assert(dw2.shape == model["w2"].shape)
    assert(dw1.shape == model["w1"].shape)
    
    model["w1"] -= alpha * dw1
    model["w2"] -= alpha * dw2
    
    return cross_entropy(cache["z"], y)

def cross_entropy(z, y):
    return - np.sum(np.log(z) * y)

def softmax(X):
    res = []
    for x in X:
        exp = np.exp(x)
        res.append(exp / exp.sum())
    return res

"""### training the model"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
# %config InlineBackend.figure_format = 'svg'
plt.style.use("seaborn")

n_iter = 600
learning_rate = 0.01

history = [backward(model, X, y, learning_rate) for _ in range(n_iter)]

plt.plot(range(len(history)), history, color="skyblue")
plt.show()

"""### With our learning rate and iteration, we see the error decrease rapidly on first iteration(may be first 10 from more than 25000 to 15000) so until it reach around 8000 and continuous to decrease very slowly for the rest of iterations untilaround 2500. We take a choice of small learning rate it is true but take big learning rate may make the error decrease fastly but it is also a  best way to make overfitting.

We can perform sanity to see which token our model predicts given the word “jus”
"""

#jus = one_hot_encode(word_to_idex["jus"], len(word_to_idex))
#result = forward(model, [jus], return_cache=False)[0]

#for word in (idex_to_word[id] for id in np.argsort(result)[::-1]):
  #print(word)

#there are which tokens predict with word mountain
mountains = one_hot_encode(word_to_idex["mountains"], len(word_to_idex))
result = forward(model, [mountains], return_cache=False)[0]

for word in (idex_to_word[id] for id in np.argsort(result)[::-1]):
    print(word)

"""#### Check embeddings"""

def get_embedding(model, word):
    try:
        idx = word_to_idex[word]
    except KeyError:
        print("`word` not in corpus")
    one_hot = one_hot_encode(idx, len(word_to_idex))
    return forward(model, one_hot)["a1"]

get_embedding(model, "mountains")

get_embedding(model, "water")

get_embedding(model, "natives")

get_embedding(model, "cannibal")

get_embedding(model, "alligators")

get_embedding(model, "surrounded")

get_embedding(model, "undergrowth")

plt.figure(figsize=(8,3))
df = pd.DataFrame({'mountains': get_embedding(model, "mountains"),
                   'water': get_embedding(model, "water"),
                   'natives': get_embedding(model, "natives"),
                    'cannibal': get_embedding(model, "cannibal"),
                    'alligators': get_embedding(model, "alligators"),
                    'surrounded': get_embedding(model, "surrounded"),
                    'undergrowth': get_embedding(model, "undergrowth"),
                  })
sns.heatmap(df.T, fmt="g", cmap='RdGy')
plt.show()

"""In this table we can easily see if words can have same context for certains position more the ressemblance with color we can have similar context.(e.g. native, water, surrounded)

The representative vectors of those words are define below.
"""

print("mountains:", get_embedding(model, "mountains"))

print("water:", get_embedding(model, "water"))

print("natives:", get_embedding(model, "natives"))

print("cannibal:", get_embedding(model, "cannibal"))

print("alligators:", get_embedding(model, "alligators"))

print("surrounded:", get_embedding(model, "surrounded"))

print("undergrowth:", get_embedding(model, "undergrowth"))

"""## **Evaluation of word embedding model: Intresec**

## An intrinsec evaluation example is correlation evaluation, the similarity between 2 words on a fixed scale.


Intrinsec evaluation method assess how well the word embedings capture the semantic or syntactic relationship between words.
 The semantic refer to the meaning of the word while syntactic refer to the grammar. Through analogy we can test it.(with syntactic such as plural, tenses..., semantic analogy for example what the word mountains is for water) but sometimes, the analogy method is not perfect because it can fail to capture completely relationship between words.


 We can also use clustering algorithm to perform intrinsec evaluation, thus cluster similar word embedding vectors or perplexity on those vectors as we do below.
"""

N=0
for sentence in model:
  words = sentence.split()
  N += len(words) +1

perplexity = 0
for score in text:
  perplexity +=  df
perplexity /= N
perplexity = 2**(-1*perplexity)

print(perplexity)

"""When looking on the result above we see that the value of our vector are 0 or infinity.

0 means good model but because of some infinity were add that can explain that the sample we took on our dataset were may set off different text.

Visualization of our tokens(specifictly vocabulary) in dataframe form.
"""

list_word = [] # start with an empty list 
for word in set(tokens):
  ab=list_word.append(list(get_embedding(model, word))) # appending word_vec correspondent to the word

data = pd.DataFrame(data = np.array(list_word).T,columns=list(set(tokens))) 
# df is dataframe 10 rows and 349 columns(vocabulary) 
data